# local-rag
Building a local RAG application

# Ollama 
Allows us to run models locally using our own hardware

Leverageing llama4 and mxbai-embed-large